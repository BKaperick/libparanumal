
#define twoPhaseReduction(r_ip, s_ip, s_res, g_ip)			\
									\
  for(int ty=0;ty<p_RDIMY;++ty;inner1){					\
    for(int tx=0;tx<p_RDIMX;++tx;inner0){				\
      s_res[ty][tx] = r_ip;						\
    }									\
  }									\
									\
  barrier(localMemFence);						\
 									\
  for(int ty=0;ty<p_RDIMY;++ty;inner1){					\
    for(int tx=0;tx<p_RDIMX;++tx;inner0){				\
      if(tx<p_RDIMX/2) s_ip[ty][tx] += s_ip[ty][tx+p_RDIMX/2];		\
      if(tx<p_RDIMX/4) s_ip[ty][tx] += s_ip[ty][tx+p_RDIMX/4];		\
      if(tx<p_RDIMX/8) s_ip[ty][tx] += s_ip[ty][tx+p_RDIMX/8];		\
      if(tx<p_RDIMX/16)s_ip[ty][tx] += s_ip[ty][tx+p_RDIMX/16];		\
      if(tx<p_RDIMX/32)s_ip[ty][tx] += s_ip[ty][tx+p_RDIMX/32];		\
      if(tx==0) s_res[ty] = s_ip[ty][tx];				\
    }									\
  }									\
  									\
  barrier(localMemFence);						\
									\
  for(int ty=0;ty<p_RDIMY;++ty;inner1){					\
    for(int tx=0;tx<p_RDIMX;++tx;inner0){				\
      if(ty==0){							\
	if(tx<p_RDIMY/2)  s_res[tx] += s_res[tx+p_RDIMY/2];		\
	if(tx<p_RDIMY/4)  s_res[tx] += s_res[tx+p_RDIMY/4];		\
	if(tx<p_RDIMY/8)  s_res[tx] += s_res[tx+p_RDIMY/8];		\
	if(tx<p_RDIMY/16) s_res[tx] += s_res[tx+p_RDIMY/16];		\
	if(tx<p_RDIMY/32) s_res[tx] += s_res[tx+p_RDIMY/32];		\
	if(tx==0) atomicAdd(g_ip, s_res[0]);				\
      }									\
    }									\
  }
  
// y = beta*y + alpha*x
// ip = y.y 
kernel void vectorAddInnerProductKernel(const int outerDim0,
					const int   N,
					const datafloat   alpha,
					const datafloat   beta,
					const datafloat *restrict x,
				        datafloat *restrict y,
					datafloat *restrict ip){

#define p_RDIMX 32
#define p_RDIMY 8
  
  for(int b=0;b<outerDim0;++b;outer0){

    shared volatile datafloat s_ip[p_RDIMY][p_RDIMX];
    shared volatile datafloat s_res[p_RDIMY];

    exclusive datafloat res;
    
    for(int ty=0;ty<p_RDIMY;++ty;inner1){
      for(int tx=0;tx<p_RDIMX;++tx;inner0){
	const int t = tx + ty*p_RDIMX;
	int i = t + b*p_RDIMX*p_RDIMY;

	res = 0;
	while(i<N){
	  datafloat yi =  beta*y[i] + alpha*x[i];
	  res += yi*yi;
	  s_ip[ty][tx] = res;
	  y[i] = yi;
	  i += outerDim0*p_RDIMX*p_RDIMY; // scan
	}
      }
    }
    
    twoPhaseReduction(res, s_ip, s_res, ip);
  }
}

// a.b, a.c, b.b
kernel void vectorAddInnerProductKernel(const int outerDim0,
					const int   N,
					const datafloat *restrict a,
					const datafloat *restrict b,
					const datafloat *restrict c,
					datafloat *restrict ips){

#define p_RDIMX 32
#define p_RDIMY 8
  
  for(int b=0;b<outerDim0;++b;outer0){

    shared volatile datafloat s_ip[p_RDIMY][p_RDIMX];
    shared volatile datafloat s_res[p_RDIMY];

    exclusive datafloat abi, aci, bbi;
    
    for(int ty=0;ty<p_RDIMY;++ty;inner1){
      for(int tx=0;tx<p_RDIMX;++tx;inner0){
	const int t = tx + ty*p_RDIMX;
	const int i = t + b*p_RDIMX*p_RDIMY;

	abi = 0.f;
	aci = 0.f;
	bbi = 0.f;
	
	while(i<N){
	  const datafloat ai =  a[i];
	  const datafloat bi =  b[i];
	  const datafloat ci =  c[i];
	  abi += ai*bi;
	  aci += ai*ci;
	  bbi += bi*bi;
	  i += outerDim0*p_RDIMX*p_RDIMY; // scan
	}
      }
    }

    twoPhaseReduction(abi, s_ip, s_res, ip+0);
    twoPhaseReduction(aci, s_ip, s_res, ip+1);
    twoPhaseReduction(bbi, s_ip, s_res, ip+2);
  }
}
    
